{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we'll go beyond standard data analysis and actually code a machine learning algorithm from scratch. As this is our first such exercise we'll start with a classic linear model, Logistic Regression. We'll accomplish this with the following steps:\n",
    "\n",
    "1. Build a class that can fit a Logistic Regression given training data as well as make predictions on examples without labels\n",
    "2. Write a unit test that confirms our class produces the correct estimates on synthetically generated data\n",
    "3. Benchmark our class against SkLearn for speed and accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Logistic Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we do any programming, we need to first understand the optimization algorithm we'll use. As a reference, we'll be following the presentation presented here: https://tminka.github.io/papers/logreg/minka-logreg-old.pdf.\n",
    "\n",
    "The logistic loss is convex so we'll use a convex optimization process called gradient descent. In particular, we will use a variant called Newton's Method that uses the 2nd derivative of the loss function to set the learning rate. To start, here are some primitives:\n",
    "\n",
    "<b>$L = \\sum y * ln(p) + (1-y)*ln(1-p)$</b>\n",
    "\n",
    "(Note, we'll drop subscripts over the data instances to keep notation simple. All sums are over examples in the data set.)\n",
    "\n",
    "<b> $g_j = \\nabla L_j = \\sum (y-p)*x_j$</b>\n",
    "\n",
    "(This is the 1st derivative, where $j$ indicates feature dimension $j$)\n",
    "\n",
    "<b> $H_{jk} = \\sum p*(1-p)*x_j*x_k$</b>\n",
    "\n",
    "(This is the 2nd derivative w.r.t. features $j$ and $k$)\n",
    "\n",
    "Now the general form of Gradient Descent follows this function:\n",
    "\n",
    "<b>$w_{new} = w_{old} - \\nu * g$</b>\n",
    "\n",
    "Where $w$ is the weight and $\\nu$ is an appropriately chosen step size. In our case, we're going to use the inverse of the 2nd derviative matrix (the Hessian) as our learning rate. If we define $H$ as the Hessian matrix with entries $H_{jk}$ from above, and $G$ a gradient vector who's $jth$ entry is $g_j$, then our optimization problem becomes:\n",
    "\n",
    "<b>$w_{new} = w_{old} - H^{-1} * G$</b>\n",
    "\n",
    "This is what we are going to program here. Let's define a class that has the following methods:\n",
    "\n",
    "1. An __init__ method that takes in an error tolerence as a stopping criterion, as well as max number of iterations.\n",
    "2. A <b>predict</b> method that takes a given matrix X and predicts $p=(1+e^{(-X*B)})^{-1}$ for each entry\n",
    "3. A <b>compute_gradient</b> method that computes the gradient vector $G$\n",
    "4. A <b>compute_hessian</b> method that computes the Hessian. Note that the $H$ can be broken down to the following matrix multiplcation: $H=X^TQX$, where $X$ is the input matrix and $Q$ is a diagonal matrix where each entry $Q_{ii}=p_i*(1-p_i)$.\n",
    "5. An <b>update_weights</b> method that applies Newton's method to update the weights\n",
    "6. A <b>check_stop</b> method that checks whether the model has converged or the max iterations have been met\n",
    "7. A <b>fit</b> method that takes in the data and runs the gradient optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write a class with the following API\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class MyFirstLogReg(object):\n",
    "    \n",
    "    def __init__(self, tol = 10**-8, max_iterations = 20):\n",
    "        \n",
    "        self.tolerance = tol\n",
    "        self.max_iterations = max_iterations\n",
    "        self.beta = None\n",
    "        self.alpha = 0\n",
    "        \n",
    "    def predict(self, Xint):\n",
    "        '''\n",
    "        Compute probs using the inverse logit\n",
    "        - Inputs: The NxK X matrix\n",
    "        - Outputs: Vector of probs of length N\n",
    "        '''\n",
    "        \n",
    "        #First compute X*beta+alpha\n",
    "        XB = Xint.dot(self.b) \n",
    "        return (1+np.exp(-1*XB))**-1\n",
    "        \n",
    "    def compute_gradient(self, Xint, y, p):\n",
    "        '''\n",
    "        Computes the gradient vector\n",
    "        -Inputs:\n",
    "            - NxK X matrix\n",
    "            - Nx1 y (label) vector\n",
    "            - Nx1 ps vector of predictions\n",
    "        -Outputs: 1xK vector of gradients\n",
    "        '''\n",
    "        return (y-p).dot(Xint)\n",
    "        \n",
    "    def compute_hessian(self, Xint, P):\n",
    "        '''\n",
    "        computes the Hessian matrix\n",
    "        -inputs:\n",
    "            - NxK X matrix\n",
    "            - Nx1 vector of predictions\n",
    "        -outputs:\n",
    "            - KxK Hessian matrix H=X^T * Diag(Q) * X\n",
    "        '''\n",
    "        #Note, in first \n",
    "        Q = np.diag(P*(1-P))\n",
    "        return (Xint.T).dot(Q).dot(Xint)\n",
    "\n",
    "\n",
    "    def update_weights(self, Xint, y, i):\n",
    "        '''\n",
    "        Updates existing weight vector\n",
    "        -Inputs:\n",
    "            -NxK X matrix\n",
    "            -Nx1 y vector\n",
    "        -updates weights by calling predict, compute_gradient and compute_hessian\n",
    "        '''\n",
    "        p = self.predict(Xint)\n",
    "        g = self.compute_gradient(Xint, y, p)\n",
    "        H = self.compute_hessian(Xint, p)\n",
    "                \n",
    "        #Store the current weights before updating so we can check for convergence\n",
    "        self.prior_b = self.b\n",
    "        \n",
    "        #update the weights\n",
    "        self.b = self.b + np.linalg.inv(H).dot(g)\n",
    "        \n",
    "        \n",
    "    def check_stop(self):\n",
    "        '''\n",
    "        check to see if euclidean distance between old and new weights (normalized)\n",
    "        is less than the tolerance\n",
    "        \n",
    "        returns: True or False on whether stopping criteria is met\n",
    "        '''\n",
    "        b_old_norm = self.prior_b / (np.sqrt(self.prior_b.dot(self.prior_b)))\n",
    "        b_new_norm = self.b / (np.sqrt(self.b.dot(self.b)))\n",
    "        diff = b_new_norm - b_old_norm\n",
    "        return (np.sqrt(diff.dot(diff)) < self.tolerance)\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        X is the Nx(K-1) data matrix\n",
    "        Y is the labels, using {0,1} coding\n",
    "        '''\n",
    "        \n",
    "        #set initial weights - add an extra dimension for the intercept\n",
    "        self.b = np.zeros(X.shape[1] + 1)\n",
    "        \n",
    "        #Initialize the slope parameter to log(base rate/(1-base rate))\n",
    "        self.b[-1] = np.log(y.mean() / (1-y.mean()))\n",
    "        \n",
    "        #create a new X matrix that includes a column of ones for the intercept\n",
    "        Xint = np.hstack((X, np.ones((X.shape[0],1))))\n",
    "\n",
    "        for i in range(self.max_iterations):\n",
    "            #print(i)\n",
    "            self.update_weights(Xint, y, i)\n",
    "            self.beta = self.b[0:-1]\n",
    "            self.alpha = self.b[-1]\n",
    "            if self.check_stop():\n",
    "                break\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note about testing\n",
    "\n",
    "One way we can test this implementation is to generate some random data according to a logistic model, and see if our model returns the correct weights. To do this:\n",
    "\n",
    "\n",
    "* generate an NxK X matrix of random numbers\n",
    "* generate a 1xK weight vector called Beta\n",
    "* set alpha\n",
    "* Given Beta and Alpha, compute P(Y|X) using the logistic functino\n",
    "* Generate an Nx1 random sequence in [0,1], and set Y=1 if R_i < P_i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Generate a dataset with N=10k and K=5\n",
    "def gen_logistic(N, K, Beta, Alpha):\n",
    "    X = np.random.random((N,K))\n",
    "    XB = X.dot(Beta) + Alpha * np.ones(N)\n",
    "    P = (1 + np.exp(-1*XB))**-1\n",
    "    Y = (np.random.random(N) < P)\n",
    "    return X, Y\n",
    "\n",
    "K = 2\n",
    "\n",
    "Beta = 2*(np.random.random(K)-1)\n",
    "Alpha = -1\n",
    "\n",
    "X, Y = gen_logistic(1000, K, Beta, Alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our dataset, let's test out our algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr = MyFirstLogReg()\n",
    "lr.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The real Betas and Alpha are:\n",
      "[-1.54388655 -1.50403716] -1\n",
      "\n",
      "The fitted Betas and Alpha are:\n",
      "[-1.59561845 -1.5173515 ] -1.05259498591\n"
     ]
    }
   ],
   "source": [
    "print('The real Betas and Alpha are:')\n",
    "print(Beta, Alpha)\n",
    "print('')\n",
    "print('The fitted Betas and Alpha are:')\n",
    "print(lr.beta, lr.alpha)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now let's compare our fitted results to SkLearn's Logistic Regression implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-1.59561842, -1.51735138]]), array([-1.05259496]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LR_SK = LogisticRegression(C=10**10).fit(X,Y)\n",
    "LR_SK.coef_, LR_SK.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this is pretty close, though not exact at all digits of precision.  We can also see that both systems produce estimates that are far from the truth. Does this mean that the code is correct? If it is indeed correct, why might we observe the above? We can write a more robust test by running multiple draws, and seeing if on average we get the right answer. We can also increase the sample size. Doing either will be more computationally expensive. Before going there let's profile our code to see if there is a way to optimize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         243 function calls in 0.020 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        5    0.000    0.000    0.000    0.000 <ipython-input-2-5667aecc335f>:14(predict)\n",
      "        5    0.000    0.000    0.000    0.000 <ipython-input-2-5667aecc335f>:25(compute_gradient)\n",
      "        5    0.000    0.000    0.015    0.003 <ipython-input-2-5667aecc335f>:36(compute_hessian)\n",
      "        5    0.004    0.001    0.020    0.004 <ipython-input-2-5667aecc335f>:50(update_weights)\n",
      "        5    0.000    0.000    0.000    0.000 <ipython-input-2-5667aecc335f>:69(check_stop)\n",
      "        1    0.000    0.000    0.020    0.020 <ipython-input-2-5667aecc335f>:82(fit)\n",
      "        1    0.000    0.000    0.020    0.020 <string>:1(<module>)\n",
      "        2    0.000    0.000    0.000    0.000 _methods.py:43(_count_reduce_items)\n",
      "        2    0.000    0.000    0.000    0.000 _methods.py:53(_mean)\n",
      "        5    0.000    0.000    0.000    0.000 linalg.py:101(get_linalg_error_extobj)\n",
      "        5    0.000    0.000    0.000    0.000 linalg.py:106(_makearray)\n",
      "       10    0.000    0.000    0.000    0.000 linalg.py:111(isComplexType)\n",
      "        5    0.000    0.000    0.000    0.000 linalg.py:124(_realType)\n",
      "        5    0.000    0.000    0.000    0.000 linalg.py:139(_commonType)\n",
      "        5    0.000    0.000    0.000    0.000 linalg.py:198(_assertRankAtLeast2)\n",
      "        5    0.000    0.000    0.000    0.000 linalg.py:209(_assertNdSquareness)\n",
      "        5    0.000    0.000    0.000    0.000 linalg.py:458(inv)\n",
      "        1    0.000    0.000    0.000    0.000 numeric.py:148(ones)\n",
      "        5    0.000    0.000    0.000    0.000 numeric.py:414(asarray)\n",
      "        9    0.000    0.000    0.000    0.000 numeric.py:484(asanyarray)\n",
      "        1    0.000    0.000    0.000    0.000 shape_base.py:232(hstack)\n",
      "        1    0.000    0.000    0.000    0.000 shape_base.py:275(<listcomp>)\n",
      "        2    0.000    0.000    0.000    0.000 shape_base.py:9(atleast_1d)\n",
      "        5    0.000    0.000    0.008    0.002 twodim_base.py:244(diag)\n",
      "        5    0.000    0.000    0.000    0.000 {built-in method builtins.abs}\n",
      "        1    0.000    0.000    0.020    0.020 {built-in method builtins.exec}\n",
      "        5    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
      "       17    0.000    0.000    0.000    0.000 {built-in method builtins.issubclass}\n",
      "       14    0.000    0.000    0.000    0.000 {built-in method builtins.len}\n",
      "        5    0.000    0.000    0.000    0.000 {built-in method builtins.max}\n",
      "        5    0.000    0.000    0.000    0.000 {built-in method builtins.min}\n",
      "       14    0.000    0.000    0.000    0.000 {built-in method numpy.core.multiarray.array}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method numpy.core.multiarray.concatenate}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method numpy.core.multiarray.copyto}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method numpy.core.multiarray.empty}\n",
      "        6    0.008    0.001    0.008    0.001 {built-in method numpy.core.multiarray.zeros}\n",
      "        5    0.000    0.000    0.000    0.000 {method '__array_prepare__' of 'numpy.ndarray' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}\n",
      "        5    0.000    0.000    0.000    0.000 {method 'astype' of 'numpy.ndarray' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "       40    0.007    0.000    0.007    0.000 {method 'dot' of 'numpy.ndarray' objects}\n",
      "        5    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'mean' of 'numpy.ndarray' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cProfile\n",
    "cProfile.run('lr.fit(X, Y)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What method is taking the most amount of time? Looking more closely at the exact sequence of operations, what might be taking up a lot of memory or time? How can we achieve the same mathematic results, but do it in a more memory friendly way?\n",
    "\n",
    "Let's do a quick test, where we design an alternative, and hopefully faster way to compute the Hessian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hessian_slow(X, P):\n",
    "    '''\n",
    "    Copy the operations used in the class above\n",
    "    '''\n",
    "    Q = np.diag(P*(1-P))\n",
    "    return (X.T).dot(Q).dot(X)\n",
    "    \n",
    "    \n",
    "def hessian_fast(X, P):\n",
    "    '''\n",
    "    Rewrite this without using the np.diag function\n",
    "    '''\n",
    "    Q = P*(1-P)\n",
    "    XQ = X.T * Q\n",
    "    return XQ.dot(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 3: 3.03 ms per loop\n"
     ]
    }
   ],
   "source": [
    "P = 0.5 * np.ones(X.shape[0])\n",
    "%timeit hessian_slow(X, P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 17.42 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "100000 loops, best of 3: 14.5 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit hessian_fast(X, P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this make a difference? Now create a new class, same as above, but overwrite the compute_hessian method with the above faster version. We're going to do this in a very light and efficient way. Essentially we'll inheret MyFirstLogReg class, which means all of the methods in the base class are callable for this one. We can overwrite the compute_hessian method with our faster approach (note, in normal software development we'd likely just revise the original class, but we're doing it this way to show the example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "class MyFasterFirstLogReg(MyFirstLogReg):\n",
    "    \n",
    "    def compute_hessian(self, Xint, p):\n",
    "        '''\n",
    "        computes the Hessian matrix\n",
    "        -inputs:\n",
    "            - NxK X matrix\n",
    "            - Nx1 vector of predictions\n",
    "        -outputs:\n",
    "            - KxK Hessian matrix H=X^T * Diag(Q) * X\n",
    "        '''\n",
    "        Q = p*(1-p)\n",
    "        XintQ = Xint.T * Q\n",
    "        return XintQ.dot(Xint)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now rerun\n",
    "lr = MyFasterFirstLogReg()\n",
    "lr.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.59561845, -1.5173515 ]), -1.0525949859058767)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.beta, lr.alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one last speed test, let's compare our faster class to sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 loops, best of 3: 722 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit LogisticRegression().fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 4.81 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "1000 loops, best of 3: 449 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit MyFasterFirstLogReg().fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got an efficient class written, let's perform 3 different unit tests:\n",
    "* Compare the results to SkLearn. This replicates a scenario where for some reason we can't use the SkLearn package and had to write our own code. Since SkLearn does exist, we can at least benchmark against it.\n",
    "* Run it once just on a much larger data set (which should come close to the truth value.\n",
    "* Run it many times over different draws from the same data generating distribution and look at the distribution of outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#First let's generate a dataset with 1000000 examples\n",
    "K = 4\n",
    "Beta = 2*(np.random.random(K)-1)\n",
    "Alpha = -2\n",
    "\n",
    "X, Y = gen_logistic(1000000, K, Beta, Alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LR_SK = LogisticRegression(C=10**30).fit(X,Y)\n",
    "LR_Mine = MyFasterFirstLogReg()\n",
    "LR_Mine.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Truth: beta=' + str(LR_SK.coef_) + ', Alpha=' +str(Alpha))\n",
    "print('SkLearn: beta=' + str(LR_SK.coef_) + ', Alpha=' +str(Alpha))\n",
    "print('Ours: beta=' + str(LR_Mine.beta) + ', Alpha=' +str(LR_Mine.alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this test we'll check whether on average our own LR class is correct. To do this, run a loop where in each iteration, generate a random data set of size N, using the same Beta and Alpha. Then fit the model and store the weights. After this runs we can look at the distributions of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "betas = []\n",
    "alphas = []\n",
    "\n",
    "for i in range(1000):\n",
    "    X, Y = gen_logistic(10000, K, Beta, Alpha)\n",
    "    LR_Mine = MyFasterFirstLogReg()\n",
    "    LR_Mine.fit(X, Y)\n",
    "    betas.append(LR_Mine.beta)\n",
    "    alphas.append(LR_Mine.alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the histograms of each parameter distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#First put it all in a dataframe\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(betas, columns = ['b' + str(k) for k in range(K)])\n",
    "df['alpha'] = alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(b0      -0.361701\n",
       " b1      -1.151148\n",
       " b2      -0.364181\n",
       " b3      -0.227846\n",
       " alpha   -2.002393\n",
       " dtype: float64,\n",
       " array([-0.35890741, -1.1600657 , -0.35649507, -0.22792468]),\n",
       " -2)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's check the means against the truth\n",
    "df.mean(), Beta, Alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAs8AAAHfCAYAAACvTDWxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10bdfffd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "for i in range(K):\n",
    "    fig.add_subplot(2,2,i+1)\n",
    "    plt.hist(df['b'+str(i)])\n",
    "    plt.plot()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
