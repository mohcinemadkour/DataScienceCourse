{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "## Subset Selection\n",
    "\n",
    "<p>Probably the most common approach to explicit feature selection is to choose a subset of $k$ features from the total set of $p$ features. The idea scenario would be to consider every possible subset of size $k$ and choose the one that has the best out-of-sample error. Although ideal in theory, in practice, there are $\\binom{p}{k} = \\frac{p!}{k!(p-k)!}$ possible subsets of size $k$. Testing each one is often cost prohibitive when $p$ gets large.<br><br>\n",
    "\n",
    "<b><u>Forward Stepwise Selection</u></b><br>\n",
    "Forward stepwise selection uses a greedy procedure to learn a good subset of $k$ features (though not guaranteed to be the \"best\" subset). The procedure goes as follows:<br>\n",
    "<ul>\n",
    "    <li>1. Initialize: Curr_Best_Subset = { } (i.e., just the intercept)</li>\n",
    "    <li>2. Loop through each feature $j$:</li>\n",
    "        <ul>\n",
    "            <li> a. Add the feature to the current best subset, train a model</li>\n",
    "            <li> b. Get out-of-sample error on the model trained with Curr_Best_Subset+feature $j$</li>\n",
    "        </ul>\n",
    "    <li>3. Choose the feature with the best out-of-sample improvement in error </li>\n",
    "    <li>4. Add this best feature to Curr_Best_Subset, and log the out-of-sample error</li>\n",
    "    <li>5. Repeat steps 2 - 4 until stopping criterion is met.</li>\n",
    "</ul><br>\n",
    "Some possible stopping criteria are:<br>\n",
    "<ul>\n",
    "    <li>Stop at a pre-defined $k$</li>\n",
    "    <li>Stop when (Performance: Best $k$ - Performance: Best $k-1$)$<\\delta$ </li>\n",
    "    <li>Same as above, but using a 1-std error rule or a t-test to compare best $k$ vs. best $k-1$</li>\n",
    "</ul><br>\n",
    "It might not always make sense or be optimal to choose $k$ in advance. The other two stopping criteria amount to continually adding features until the feature does not improve the model, either in an absolute sense or using some statistical test to compare the difference between cross-validated means.<br>\n",
    "\n",
    "\n",
    "\n",
    "The following code shows how this can be done. We'll use the churn prediction data. For a metric, let's assume we care the most about having a good estimate of $P(Y|X)$, so we'll use logistic regression with log-loss as our feature selection criterion. Again, the log-loss is defined as:<br><br>\n",
    "<center>$logloss = -\\frac{1}{n} \\sum\\limits_{i=1}^n \\: y_i*log(p_i)+(1-y_i)*log(1-p_i)$</center><br>\n",
    "We'll also use the last stopping criteria shown above.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load the data. Not much prep is need since its a clean dataset\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append(\"C:/Users/kevin/Documents/GitHub/DS_course/ipython\")\n",
    "import course_utils as bd\n",
    "from sklearn import linear_model\n",
    "\n",
    "\n",
    "f = 'C:/Users/kevin/Documents/GitHub/DS_course/datasets/Cell2Cell_data.csv'\n",
    "dat=pd.read_csv(f, header=0, sep=',')\n",
    "train, test = bd.trainTest(dat, 0.5)\n",
    "lab = 'churndep'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "\n",
    "def runXVal_LogLoss(cv, X_sub, Y_tr):\n",
    "    '''\n",
    "    Runs LR cross validation with no regularization, returns mean and standard error of mean\n",
    "    '''\n",
    "    ll = []; \n",
    "    for train_index, test_index in cv:\n",
    "        X_tr_f = X_sub.iloc[train_index]\n",
    "        X_va_f = X_sub.iloc[test_index]\n",
    "        Y_tr_f = Y_tr.iloc[train_index]\n",
    "        Y_va_f = Y_tr.iloc[test_index]\n",
    "\n",
    "        lr = linear_model.LogisticRegression()\n",
    "        lr.fit(X_tr_f, Y_tr_f)\n",
    "        P = lr.predict_proba(X_va_f)[:,1]\n",
    "        \n",
    "        ll.append(-1*(((Y_va_f==1)*np.log(P)+(Y_va_f==0)*np.log(1-P)).mean()))\n",
    "\n",
    "    return [np.array(ll).mean(), np.array(ll).std()/np.sqrt(len(ll))]\n",
    "        \n",
    "def LrForward_LogLoss(X_tr, Y_tr, cv):\n",
    "    '''\n",
    "    Runs cross-validated stepwise selection\n",
    "    Does not pick the best features, but returns data\n",
    "    Returns a dictionary that shows at each k: [feature set], x-validated mean, x-validated var \n",
    "    For each loop, chooses the feature with best mean+1stderr\n",
    "    '''\n",
    "    results = {}\n",
    "    curr_best = set([])\n",
    "    cand_list = set(X_tr.columns.values)\n",
    "    k = 1\n",
    "    \n",
    "    while (len(cand_list)>0):\n",
    "        best_mu = 10**10; best_serr = 10**10; \n",
    "        for f in cand_list:\n",
    "            use_x = list(curr_best)+[f]\n",
    "            mu, serr = runXVal_LogLoss(cv, X_tr[use_x], Y_tr)\n",
    "            if ((mu + serr) < (best_mu + best_serr)):\n",
    "                best_mu = mu\n",
    "                best_serr = serr\n",
    "                best_f = f\n",
    "        curr_best.add(best_f) #Add the best feature to the curr_best_set\n",
    "        cand_list = cand_list.difference(curr_best) #Remove the best feature from the candidate set\n",
    "        results[k] = [list(curr_best), best_mu, best_serr]\n",
    "        k+=1\n",
    "        \n",
    "    return results\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Run the forward selection\n",
    "cv = KFold(n=train.shape[0], n_folds = 10)\n",
    "r = LrForward_LogLoss(train.drop(lab,1), train[lab], cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now plot the incremental results\n",
    "ks = []; mus = []; serrs = [];\n",
    "for i in range(len(r.keys())):\n",
    "    ks.append(i+1)\n",
    "    mus.append(r[i+1][1])\n",
    "    serrs.append(r[i+1][2])\n",
    "\n",
    "    \n",
    "best_1serr = min(np.array(mus) + np.array(serrs))\n",
    "plt.clf()\n",
    "plt.plot(ks, mus, 'b.-', label = 'LL of Set k')\n",
    "plt.plot(ks, np.array(mus) + np.array(serrs), 'k+-')\n",
    "plt.plot(ks, np.array(mus) - np.array(serrs), 'k--')\n",
    "plt.plot(ks, np.ones(len(ks))*best_1serr, 'r', label ='1 std err rule')\n",
    "\n",
    "plt.xlim([1,11])\n",
    "\n",
    "plt.title('Stepwise Forward Feature Selection')\n",
    "plt.xlabel('Greedily Selected Subset of Size k')\n",
    "plt.ylabel('X Validated LogLoss')\n",
    "    \n",
    "plt.legend(loc=1, ncol=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The above plot will support any of the stopping criteria defined above. Assuming we haven't predefined $k$, we might choose $k=7$, as that is where the error really seems to plateau. Additionally, we could be more conservative. The red line shows the min value of (mean+1 std error). Any value of $k$ that is less than this value is statistically the same as the $k$ with the lowest mean error. If we were using that rule, we'd potentially stop at $k=3$.<br><br>\n",
    "</p>\n",
    "\n",
    "## SVD Based Dimensionality Reduction\n",
    "\n",
    "<p>Another way to bring the learning down to $k$ features is to find a projection matrix that produces a rank-$k$ approximation of our training data $X$. The best way to get a low-rank approximation, from a signal preservation perspective, is by the use of the Singular Value Decomposition. A review of the SVD can be found here:<br><br>\n",
    "http://nbviewer.ipython.org/github/briandalessandro/DataScienceCourse/blob/master/ipython/Lecture3_PhotoSVD.ipynb\n",
    "<br><br>\n",
    "We'll put it to work here by doing the following:\n",
    "<ul>\n",
    "    <li>Decomposing our training data</li>\n",
    "    <li>For each rank-k approximation of X, get cross-validated error</li>\n",
    "    <li>Compare this error to the Stepwise Forward Feature Selection above</li></ul><br>\n",
    "The scale of the features influences the spectrum of the singular values, so we'll normalize the feature so they all have the same scale.\n",
    "</p>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "\n",
    "f = '/Users/briand/Desktop/ds course/hw/Cell2Cell_data.csv'\n",
    "\n",
    "X_train = train.drop(lab, 1)\n",
    "X_test = test.drop(lab, 1)\n",
    "Y_train = train[lab]\n",
    "Y_test = test[lab]\n",
    "\n",
    "X_train_norm = pd.DataFrame(scale(X_train, axis=0, with_mean=True, with_std=True, copy=True), columns = X_train.columns.values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now let's decompose the training data.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "U, sig, Vt = np.linalg.svd(X_train_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Out of curiosity, let's plot the spectrum to get a sense of how independent the features are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(sig**2)/np.sum(sig**2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We can see above that the first 5 or 6 singular values account for 70-80% of the sum of squares of our data. Now let's see how a rank-$k$ approximation does in cross-validation.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def LrRankK_LogLoss(U, sig, Vt, Y_tr, cv):\n",
    "    '''\n",
    "    Runs cross-validation on each rank k approximation\n",
    "    '''\n",
    "    results = {}\n",
    "    \n",
    "    for k in range(len(sig)):\n",
    "        X_k = pd.DataFrame(np.matrix(U[:, :k+1]) * np.diag(sig[:k+1]))\n",
    "        mu, serr = runXVal_LogLoss(cv, X_k, Y_tr)\n",
    "        results[k+1] = [mu, serr]\n",
    "        \n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r_svd = LrRankK_LogLoss(U, sig, Vt, Y_train, cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now let's plot the results. We'll also plot it with the stepwise forward selection results above.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "\n",
    "k_svd = []\n",
    "mu_svd = []\n",
    "serr_svd = []\n",
    "\n",
    "for i in range(len(r_svd.keys())):\n",
    "    k_svd.append(i+1)\n",
    "    mu_svd.append(r_svd[i+1][0])\n",
    "    serr_svd.append(r_svd[i+1][1])\n",
    "    \n",
    "plt.plot(k_svd, mu_svd, 'b.-', label = 'SVD Rank-k Proj')\n",
    "plt.plot(k_svd, np.array(mu_svd)+np.array(serr_svd), 'b+')\n",
    "plt.plot(k_svd, np.array(mu_svd)-np.array(serr_svd), 'b--')\n",
    "\n",
    "\n",
    "plt.plot(ks, mus, 'r.-', label = 'Stepwise Forward')\n",
    "plt.plot(ks, np.array(mus) + np.array(serrs), 'r+-')\n",
    "plt.plot(ks, np.array(mus) - np.array(serrs), 'r--')\n",
    "\n",
    "plt.legend(loc=1, ncol=2)\n",
    "\n",
    "plt.title('Stepwise Forward Feature Selection vs SVD Dim Reduction')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('X Validated LogLoss')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>This is a very interesting plot. We can see that SVD approach does almost universally worse than the stepwise approach. Its not until we get to the last two features do we see them equal each other. <br><br>\n",
    "My interpretation of this is that the SVD approach does not reduce the feature space with the outcome or learning task in mind. It is purely untangling the covariance structure of the matrix $X$. It is quite possible (based on the evidence above), that the most predictive features corresponded to the lowest singular values. That is why we don't see a large reduction in the cross-validated error until the last few columns are added into the matrix.<br><br>\n",
    "\n",
    "This is not to say or prove that dimensionality reduction with SVD is a generally inferior approach. Because the matrix $X$ is dense and not very high dimensional, the SVD approach just might not be appropriate here. It is also slower, as the cost of generating the SVD isn't always cheap.\n",
    "\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
