{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reciever Operator Curve\n",
    "<p>The Reciever Operator Curve is a useful tool for understanding how well a classification model ranks instances. <br><br> \n",
    "\n",
    "<b><u>Defining the ROC</u></b><br><br>\n",
    "Assume some function $f(x)$ that gives real-valued output. We define a classifier based on $f(x)$ as the following:<br><br>\n",
    "<center>$c(x) = I(f(x)\\geq t)$</center><br><br>\n",
    "for some constant $t$, and where $I(...)$ is the indicator function. For each threshold $t$ that is chosen, we can define two quantities: 1). The false positive rate, $FPR = \\frac{False Positives}{False Positives+True Negatives}$, and 2). The true positive rate, $TPR = \\frac{True Positives}{True Positives+False Negatives}$. The ROC is the result of plotting $FPR$ against $TPR$ for each value of $t$ that is possible in the data.<br><br>\n",
    "\n",
    "<b><u>The Area Under the Curve</u></b><br><br>\n",
    "The area under the curve captures the ROC information as a single metric. It is useful for both model and feature selection. With the ROC defined, we simply integrate across all values of FPR to get the area. <br><br>\n",
    "<center>$AUC = \\int_0^1 \\: TPR \\:dFPR$</center><br><br>\n",
    "\n",
    "While this is the exact definition, and is how one might measure it empirically using numeric integration, it is not necessarily intuitive to think of a classifier in the $FPR$ domain. After all, we choose the threshold $t$ and not the $FPR$. We can get around this, but first lets define a few things. Let $s^+(v)$ and $s^-(v)$ be the PDF of $v=f(x)$ for positive and negative labeled examples, respectively. Similarly, let $S^+(v)$ and $S^-(v)$ be the CDF of $v=f(x)$ for positive and negative labeled labeled, respectively.\n",
    "We can use these to define the $FPR$ and $TPR$ as functions of $t$.<br><br>\n",
    "\n",
    "<center>$FPR(t) = \\int_t^\\infty \\:s^-(v) \\:dv=1-S^-(t)$</center>\n",
    "<center>$TPR(t) = \\int_t^\\infty \\:s^+(v) \\:dv=1-S^+(t)$</center>\n",
    "\n",
    "\n",
    "<br><br>\n",
    "With $FPR$ and $TPR$ defined as functions of $t$, we can use the fact that $\\frac{dFPR(t)}{dt}=-s^-(t)$\n",
    "to show that:\n",
    "<br><br>\n",
    "<center>$AUC = \\int_0^1 \\: TPR(t) \\:dFPR(t) =-\\int_{\\infty}^{-\\infty} \\:TPR(t) \\:s^-(t)\\:dt = \\int_{-\\infty}^{\\infty} \\:TPR(t) \\:s^-(t)\\:dt=\\int_{-\\infty}^{\\infty}\\:(1-S^+(t))\\:s^-(t)\\:dt$</center>\n",
    "\n",
    "<br><br>\n",
    "This last term expresess $AUC$ as a function of both the distribution of $f(x)$ for both positive and negative instances. This gives the AUC a probabilistic interpretation. For a given threshold $t$, we can choose one negative with a score exactly $t$. This has a probability of $s^-(t)$. The probability that a positive will have a score greater than this negative is $TPR(t)=1-S^+(t)$. If we repeat this exercise for every value of $t$, the probability that a randomly drawn positve instance has a higher score than a randomly drawn negative instance is then the $AUC$. I.e.,<br><br>\n",
    "\n",
    "<center>$AUC = P(f(x^+)>f(x^-)) = \\int_{-\\infty}^{\\infty}\\:P(f(x^-)=t)*P(f(x^+)\\geq t)\\:dt=\\int_{-\\infty}^{\\infty}\\:(1-S^+(t))\\:s^-(t)\\:dt$</center>\n",
    "<br><br><br></p>\n",
    "\n",
    "### Putting the ROC to Work\n",
    "\n",
    "<p>\n",
    "So that's enough theory for now, let's do a model bakeoff and show how the ROC can be used for model selection. First we'll build a model using four different algorithms and compare their ROCs.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=10, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Build the datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm, linear_model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import sys\n",
    "sys.path.append(\"C:/Users/kevin/Documents/GitHub/DS_course/ipython\")\n",
    "import course_utils as bd\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import imp\n",
    "imp.reload(bd)\n",
    "\n",
    "#Load data and downsample for a 50/50 split, then split into a train/test\n",
    "f='C:/Users/kevin/Documents/GitHub/DS_course/datasets/ads_dataset_cut.txt'\n",
    "\n",
    "train_split = 0.5\n",
    "tdat = pd.read_csv(f,header=0,sep='\\t')\n",
    "\n",
    "lab = 'y_buy'\n",
    "\n",
    "moddat = bd.downSample(tdat,lab,9)\n",
    "#We know the dataset is sorted so we can just split by index\n",
    "train = moddat[:int(math.floor(moddat.shape[0]*train_split))]\n",
    "test = moddat[int(math.floor(moddat.shape[0]*train_split)):]\n",
    "\n",
    "#Train the models\n",
    "dt = DecisionTreeClassifier(criterion='entropy',min_samples_leaf = 10,max_depth = 4)\n",
    "dt = dt.fit(train.drop(lab,1),train[lab])\n",
    "\n",
    "lr = linear_model.LogisticRegression(C=1000)\n",
    "lr.fit(train.drop(lab,1), train[lab])\n",
    "\n",
    "mm = svm.SVC(kernel='linear', C=1)\n",
    "mm.fit(train.drop(lab,1), 2*train[lab]-1)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=10, p=2)\n",
    "knn.fit(train.drop(lab,1), train[lab])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plt.clf()\n",
    "\n",
    "def plotAUC(truth, pred, lab):\n",
    "    fpr, tpr, thresholds = roc_curve(truth, pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    c = (np.random.rand(), np.random.rand(), np.random.rand())\n",
    "    plt.plot(fpr, tpr, color=c, label= lab+' (AUC = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('FPR')\n",
    "    plt.ylabel('TPR')\n",
    "    plt.title('ROC')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "plotAUC(test[lab], dt.predict_proba(test.drop(lab,1))[:,1], 'DT')\n",
    "plotAUC(test[lab], lr.predict_proba(test.drop(lab,1))[:,1], 'LR')    \n",
    "plotAUC(test[lab], mm.decision_function(test.drop(lab,1)), 'SVM')    \n",
    "plotAUC(test[lab], knn.predict_proba(test.drop(lab,1))[:,1], 'kNN')    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>A calibration chart</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getMAE(pred, truth):\n",
    "    return np.abs(truth - pred).mean()\n",
    "\n",
    "def getLL(pred, truth):\n",
    "    ll_sum = 0\n",
    "    for i in range(len(pred)):\n",
    "        if (pred[i] == 0):\n",
    "            p = 0.0001\n",
    "        elif (pred[i] == 1):\n",
    "            p = 0.9999\n",
    "        else:\n",
    "            p = pred[i]\n",
    "        ll_sum += truth[i]*np.log(p)+(1-truth[i])*np.log(1-p)\n",
    "    return (ll_sum)/len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def plotCalib(truth, pred, bins, f, l):\n",
    "    mae = np.round(getMAE(pred, truth),3)\n",
    "    ll = np.round(getLL(pred, truth), 3)\n",
    "    d = pd.DataFrame({'p':pred, 'y':truth})\n",
    "    d['p_bin'] = np.floor(d['p']*bins)/bins\n",
    "    d_bin=d.groupby(['p_bin']).agg([np.mean,len])\n",
    "    filt = (d_bin['p']['len']>f)\n",
    "    plt.plot(d_bin['p']['mean'][filt], d_bin['y']['mean'][filt], 'b.', label=l+': '+'ll={},mae={}'.format(ll,mae))\n",
    "    plt.plot([0.0, 1.0], [0.0, 1.0], 'k-')\n",
    "    #plt.title(label+':'+'MAE={}'.format(mae, ll))\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('prediction P(Y|X)')\n",
    "    plt.ylabel('actual P(Y|X)')\n",
    "    plt.legend(loc=4)\n",
    "    \n",
    "plt.clf()\n",
    "fig = plt.figure()\n",
    "#plt.title('Calibration Charts for DT vs. LR')\n",
    "\n",
    "ax = fig.add_subplot(2,1,1)\n",
    "plotCalib(test[lab].values, dt.predict_proba(test.drop(lab,1))[:,1], 50, 10, 'DT')\n",
    "\n",
    "ax = fig.add_subplot(2,1,2)\n",
    "plotCalib(test[lab].values, lr.predict_proba(test.drop(lab,1))[:,1], 50, 10, 'LR')    \n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Loop through features to get AUC and LL</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def evalFeat(x_train, y_train, x_test, y_test):\n",
    "    lr_f = linear_model.LogisticRegression(C=1e30)\n",
    "    lr_f.fit(x_train, y_train)\n",
    "    p = lr_f.predict_proba(x_test)[:,1]\n",
    "    ll = -1*getLL(p, y_test.values)\n",
    "    auc = roc_auc_score(y_test, p)\n",
    "    return [ll, auc]\n",
    "\n",
    "\n",
    "lls = []\n",
    "aucs = []\n",
    "feats = train.drop(lab,1).columns.values\n",
    "for f in feats:\n",
    "    ll_f, auc_f = evalFeat(train[[f]], train[lab], test[[f]], test[lab])\n",
    "    lls.append(ll_f)\n",
    "    aucs.append(auc_f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>A quick Lift chart</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def liftTable(pred, truth, b):\n",
    "    df = pd.DataFrame({'p':pred+np.random.rand(len(pred))*0.000001,'y':truth})\n",
    "    df['b'] = b - pd.qcut(df['p'], b, labels=False)\n",
    "    df['n'] = np.ones(df.shape[0])\n",
    "    df_grp = df.groupby(['b']).sum()\n",
    "    base = np.sum(df_grp['y'])/float(df.shape[0])\n",
    "    df_grp['n_cum'] = np.cumsum(df_grp['n'])/float(df.shape[0])\n",
    "    df_grp['y_cum'] = np.cumsum(df_grp['y'])\n",
    "    df_grp['p_y_b'] = df_grp['y']/df_grp['n']\n",
    "    df_grp['lift_b'] = df_grp['p_y_b']/base\n",
    "    df_grp['cum_lift_b'] = (df_grp['y_cum']/(float(df.shape[0])*df_grp['n_cum']))/base\n",
    "    return df_grp\n",
    "    \n",
    "\n",
    "lifts_lr = liftTable(lr.predict_proba(test.drop(lab,1))[:,1], test[lab], 25)\n",
    "#lifts_svm = liftTable(mm.decision_function(test.drop(lab,1)), test[lab], 25)\n",
    "lifts_dt = liftTable(dt.predict_proba(test.drop(lab,1))[:,1]+np.random.rand(test.shape[0])*0.00001, test[lab], 25)\n",
    "lifts_knn = liftTable(knn.predict_proba(test.drop(lab,1))[:,1], test[lab], 25)   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.title('A Lift Chart on Ads Data with LR')\n",
    "plt.plot(lifts_lr['n_cum'], lifts_lr['cum_lift_b'], label='LR')\n",
    "plt.plot(lifts_knn['n_cum'], lifts_knn['cum_lift_b'], label='kNN')\n",
    "plt.plot(lifts_dt['n_cum'], lifts_dt['cum_lift_b'], label='DT')\n",
    "\n",
    "plt.plot(lifts_lr['n_cum'], np.ones(lifts_lr.shape[0]))\n",
    "plt.xlim([lifts_lr['n_cum'].min(), lifts_lr['n_cum'].max()])\n",
    "plt.ylim([0.0, lifts_lr['cum_lift_b'].max()+1])\n",
    "plt.xlabel('Cumulative Ranked Users')\n",
    "plt.ylabel('Cumulative Lift')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
