{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>This notebook is a demonstration of two types of feature engineering methods:<br><br>\n",
    "<ul>\n",
    "    <li>Creating non-linear transformations of individual features</li>\n",
    "    <li>Transforming continuous or discrete features in binary bins - aka \"binning\"</li>\n",
    "</ul><br><br>\n",
    "\n",
    "The first thing we do is write a few functions that make transformations.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Binning vs transformation\n",
    "'''\n",
    "\n",
    "from sklearn import linear_model\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def genY(x, err, betas):\n",
    "    '''\n",
    "    Goal: generate a Y variable as Y=XB+e \n",
    "    Input\n",
    "    1. an np array x of length n\n",
    "    2. a random noise vector r of length n\n",
    "    3. a (d+1) x 1 vector of coefficients b - each represents ith degree of x\n",
    "    '''\n",
    "    d = pd.DataFrame(x, columns=['x'])    \n",
    "    y = err\n",
    "    for i,b in enumerate(betas):\n",
    "        y = y + b*x**i\n",
    "    d['y'] = y\n",
    "    return d\n",
    "\n",
    "\n",
    "def makePolyFeat(d, deg):\n",
    "    '''\n",
    "    Goal: Generate features up to X**deg\n",
    "    1. a data frame with two features X and Y\n",
    "    4. a degree 'deg' (from which we make polynomial features \n",
    "    \n",
    "    '''\n",
    "    #Generate Polynomial terms\n",
    "    for i in range(2, deg+1):\n",
    "        d['x'+str(i)] = d['x']**i\n",
    "    return d\n",
    "\n",
    "\n",
    "\n",
    "def makeBin(d, bins):\n",
    "    '''\n",
    "    This takes in a dataframe with a feature X and makes evenly spaced bin features\n",
    "    using the pandas get_dummies function\n",
    "    '''\n",
    "    d['g'] = np.floor(bins*(d['x']-d['x'].min())/(d['x'].max()-d['x'].min())).astype(int)\n",
    "    d['g']=-1*(d['g']==bins)+d['g'] #Puts the highest entry into the right bin\n",
    "    #Note that the get_dummies function makes k dummy features if there are k\n",
    "    #discrete values. In modeling you should always use k-1 bins\n",
    "    dummies = pd.get_dummies(d['g'], prefix='bin')\n",
    "    d_m = pd.merge(d, dummies, left_index=True, right_index=True, how='inner')\n",
    "    del d_m['g'] #we don't need this\n",
    "    del d_m['bin_0'] #we don't this either\n",
    "    return d_m\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now that we've created the functions, let's generate some data. Again, the goal is to generate an X-Y relationship with noise, but where we know the underlying data generating distribution.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "betas = [0, 4, -3.5, 1]\n",
    "n=200\n",
    "sig=2.2\n",
    "sp=20\n",
    "\n",
    "x_init = np.random.uniform(0,1,n)\n",
    "e_init = np.random.normal(0, sig, n)\n",
    "\n",
    "dat = genY(x_init, e_init, betas)\n",
    "dat = makePolyFeat(dat, 6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now we want to see the effect of fitting polynomial curves of different degrees to our noisy data set. Ultimately, we want to illustrate how model specification (and feature engineering) affects the bias-variance tradeoff.\n",
    "\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def PlotLinDeg(X_train, y_train, X_test, y_test, i, t):\n",
    "    '''\n",
    "    This function builds a regression model on the simulated data\n",
    "    1. plots the test data\n",
    "    2. plots the fitted line\n",
    "    3. Shows sum-square error\n",
    "    '''\n",
    "    regr = linear_model.LinearRegression(fit_intercept=True)\n",
    "    regr.fit(X_train, y_train)\n",
    "    y_hat=regr.predict(X_train)\n",
    "    y_hat_test=regr.predict(X_test)\n",
    "    ss_train=((y_train-y_hat)**2).mean()\n",
    "    ss_test=((y_test-y_hat_test)**2).mean()\n",
    "    #Plot train X vs. Predicted Y_train\n",
    "    plt.subplot(2, 3, i)\n",
    "    plt.plot(X_train['x'], y_train, 'b.')\n",
    "    plt.plot(X_train['x'], y_hat, 'r.')\n",
    "    plt.title('{}\\n Train MSE={}'.format(t,round(ss_train,4)))\n",
    "    #Plot test X vs. Predicted Y_test\n",
    "    plt.tick_params(axis='x', which='both', bottom='off', top='off', labelbottom='off')\n",
    "    plt.subplot(2,3,i+3)\n",
    "    plt.plot(X_test['x'], y_test, 'b.')\n",
    "    plt.plot(X_test['x'], y_hat_test, 'r.')\n",
    "    plt.title('Test MSE={}'.format(round(ss_test,4)))\n",
    "    plt.tick_params(axis='x', which='both', bottom='off', top='off', labelbottom='off')\n",
    "    \n",
    "    \n",
    "def PlotLinBin(X_train, y_train, X_test, y_test, i, t, x, x_t):\n",
    "    '''\n",
    "    This function builds a regression model on the simulated data\n",
    "    1. plots the test data\n",
    "    2. plots the fitted line\n",
    "    3. Shows sum-square error\n",
    "    '''\n",
    "    regr = linear_model.LinearRegression(fit_intercept=True)\n",
    "    regr.fit(X_train, y_train)\n",
    "    y_hat=regr.predict(X_train)\n",
    "    y_hat_test=regr.predict(X_test)\n",
    "    ss_train=((y_train-y_hat)**2).mean()\n",
    "    ss_test=((y_test-y_hat_test)**2).mean()\n",
    "    #Plot train X vs. Predicted Y_train\n",
    "    plt.subplot(2, 3, i)\n",
    "    plt.plot(x, y_train, 'b.')\n",
    "    plt.plot(x, y_hat, 'r.')\n",
    "    plt.title('{}\\n Train MSE={}'.format(t,round(ss_train,4)))\n",
    "    #Plot test X vs. Predicted Y_test\n",
    "    plt.tick_params(axis='x', which='both', bottom='off', top='off', labelbottom='off')\n",
    "    plt.subplot(2,3,i+3)\n",
    "    plt.plot(x_t, y_test, 'b.')\n",
    "    plt.plot(x_t, y_hat_test, 'r.')\n",
    "    plt.title('Test MSE={}'.format(round(ss_test,4)))\n",
    "    plt.tick_params(axis='x', which='both', bottom='off', top='off', labelbottom='off')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sp = 20\n",
    "\n",
    "f1=['x']; f3=['x', 'x2', 'x3']; f6=['x', 'x2', 'x3', 'x4', 'x5', 'x6']\n",
    "\n",
    "fig=plt.figure()\n",
    "\n",
    "PlotLinDeg(dat[f1][:sp], dat['y'][:sp], dat[f1][sp:], dat['y'][sp:], 1, 'Degree 1')\n",
    "PlotLinDeg(dat[f3][:sp], dat['y'][:sp], dat[f3][sp:], dat['y'][sp:], 2, 'Degree 3')\n",
    "PlotLinDeg(dat[f6][:sp], dat['y'][:sp], dat[f6][sp:], dat['y'][sp:], 3, 'Degree 6')\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The above plot is a great illustration of bias-variance tradeoffs. The top row shows an nth-degree model fit to a sparse training set. The bottom row shows this fitted model against the test data (with actuals in blue and predicted in red). We can see multiple things here:<br>\n",
    "<ul>\n",
    "    <li>MSE gets much better on training data as we increase the degree to 6</li>\n",
    "    <li>MSE gets notably worse on test data as we increase the degree from 3 to 6</li>\n",
    "    <li>A linear model is notably biased (has the worst training error), but the error from bias is better than the error induced from overfitting (i.e., the high variance model)</li>\n",
    "</ul><br>\n",
    "\n",
    "Now let's do a similar experiment with the bin features.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getBinName(vl, pre):\n",
    "    l=[]\n",
    "    for v in vl:\n",
    "        if (pre in v):\n",
    "            l.append(v)\n",
    "    return l\n",
    "\n",
    "\n",
    "d4 = makeBin(dat,2); g4 = getBinName(d4.columns.values,'bin')\n",
    "PlotLinBin(d4[g4][:sp], d4['y'][:sp], d4[g4][sp:], d4['y'][sp:], 1, 'Bins=10', d4['x'][:sp], d4['x'][sp:])\n",
    "\n",
    "d10=makeBin(dat, 10); g10 = getBinName(d10.columns.values, 'bin')\n",
    "PlotLinBin(d10[g10][:sp], d10['y'][:sp],d10[g10][sp:], d10['y'][sp:], 2, 'Bins=30', d10['x'][:sp], d10['x'][sp:])\n",
    "\n",
    "d50=makeBin(dat, 20); g50=getBinName(d50.columns.values, 'bin')\n",
    "PlotLinBin(d50[g50][:sp], d50['y'][:sp],d50[g50][sp:], d50['y'][sp:], 3, 'Bins=50', d50['x'][:sp], d50['x'][sp:])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
